# RL

![image-20220717155617206](/Users/anmingyu/Github/AlgorithmsUnion/Reinforcement -Learning/image-20220717155617206.png)

对于一般的有监督学习任务，我们的目标是找到一个最优的模型函数，使其在训练数据集上最小化一个给定的损失函数。在训练数据独立同分布的假设下，这个优化目标表示最小化模型在整个数据分布上的泛化误差（generalization error），用简要的公式可以概括为：



相比之下，强化学习任务的最终优化目标是最大化智能体策略在和动态环境交互过程中的价值。根据1.5节的分析，策略的价值可以等价转换成奖励函数在策略的占用度量上的期望，即：

- 有监督学习和强化学习的优化目标相似，即都是在优化某个数据分布下的一个分数值的期望。
- 二者优化的途径是不同的，有监督学习直接通过优化模型对于数据特征的输出来优化目标，即修改目标函数而数据分布不变；强化学习则通过改变策略来调整智能体和环境交互数据的分布，进而优化目标，即修改数据分布而目标函数不变。

综上所述，一般有监督学习和强化学习的范式之间的区别为：

- 一般的有监督学习关注寻找一个模型，使其在给定数据分布下得到的损失函数的期望最小；
- 强化学习关注寻找一个智能体策略，使其在与动态环境交互的过程中产生最优的数据分布，即最大化该分布下一个给定奖励函数的期望。



每人一模型

