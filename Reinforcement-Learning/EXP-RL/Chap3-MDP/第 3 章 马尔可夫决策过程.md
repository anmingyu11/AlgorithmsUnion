# 第 3 章 马尔可夫决策过程

## 3.1 简介

**马尔可夫决策过程**（Markov decision process，MDP）是强化学习的重要概念。

要学好强化学习，我们首先要掌握马尔可夫决策过程的基础知识。

前两章所说的强化学习中的环境一般就是一个马尔可夫决策过程。

与多臂老虎机问题不同，马尔可夫决策过程包含状态信息以及状态之间的转移机制。

如果要用强化学习去解决一个实际问题，第一步要做的事情就是把这个实际问题抽象为一个马尔可夫决策过程，也就是明确马尔可夫决策过程的各个组成要素。

本章将从马尔可夫过程出发，一步一步地进行介绍，最后引出马尔可夫决策过程。

## 3.2 马尔可夫过程

### 3.2.1 随机过程

**随机过程**（stochastic process）是概率论的“动力学”部分。概率论的研究对象是静态的随机现象，而随机过程的研究对象是随时间演变的随机现象（例如天气随时间的变化、城市交通随时间的变化）。

在随机过程中，随机现象在某时刻 $t$ 的取值是一个向量随机变量，用 $S_t$ 表示，所有可能的状态组成状态集合 $S$。随机现象便是状态的变化过程。在某时刻 $t$ 的状态 $S_t$ 通常取决于 $t$ 时刻之前的状态。我们将已知历史信息 $\left(S_{1}, \ldots, S_{t}\right)$ 时下一个时刻状态 $S_{t+1}$ 为的概率表示成 $P\left(S_{t+1} \mid S_{1}, \ldots, S_{t}\right)$。

### 3.2.2 马尔可夫性质

当且仅当某时刻的状态只取决于上一时刻的状态时，一个随机过程被称为具有**马尔可夫性质**（Markov property），用公式表示为 $P\left(S_{t+1} \mid S_{t}\right)=P\left(S_{t+1} \mid S_{1}, \ldots, S_{t}\right)$。

也就是说，当前状态是未来的充分统计量，即下一个状态只取决于当前状态，而不会受到过去状态的影响。

需要明确的是，具有马尔可夫性并不代表这个随机过程就和历史完全没有关系。

因为虽然时刻的状态只与时刻的状态有关，但是时刻的状态其实包含了时刻的状态的信息，通过这种链式的关系，历史的信息被传递到了现在。

马尔可夫性可以大大简化运算，因为只要当前状态可知，所有的历史信息都不再需要了，利用当前状态信息就可以决定未来。

### 3.2.3 马尔可夫过程

**马尔可夫过程**（Markov process）指具有马尔可夫性质的随机过程，也被称为**马尔可夫链**（Markov chain）。

我们通常用元组 $\langle\mathcal{S}, \mathcal{P}\rangle$ 描述一个马尔可夫过程，其中 $\mathcal{S}$ 是有限数量的状态集合，$\mathcal{P}$是**状态转移矩阵**（state transition matrix）。

假设一共有 $n$ 个状态，此时 $\mathcal{S}=\left\{s_{1}, s_{2}, \ldots, s_{n}\right\}$。

状态转移矩阵 $\mathcal{P}$ 定义了所有状态对之间的转移概率，即
$$
\mathcal{P}=\left[\begin{array}{ccc}
P\left(s_{1} \mid s_{1}\right) & \cdots & P\left(s_{n} \mid s_{1}\right) \\
\vdots & \ddots & \vdots \\
P\left(s_{1} \mid s_{n}\right) & \cdots & P\left(s_{n} \mid s_{n}\right)
\end{array}\right]
$$
矩阵 $\mathcal{P}$ 中第 $i$  行第 $j$ 列元素表示从状态转移到状态的概率，我们称为状态转移函数。从某个状态出发，到达其他状态的概率和必须为 1，即状态转移矩阵的每一行的和为 1。